{
  "embed_model": "BAAI/bge-small-en-v1.5",
  "chunk_size": 900,
  "chunk_overlap": 150,
  "chunks": [
    "UDAY VARIKUPPALA \nMemphis (open to relocation) • (901) 633-5949 • uday.v3669@gmail.com \n \nPROFESSIONAL SUMMARY \nData Engineer and Analyst with 2+ years of experience building scalable data systems that power everything \nfrom live dashboards to machine learning in production. Work sits at the intersection of analytics, \nengineering, and business enabling teams to shift from reactive reporting to predictive, automated decision-\nmaking. \nDelivered measurable impact across fintech, healthcare, and enterprise environments by transforming high-\nvolume, unstructured data into trusted, actionable pipelines. Expertise spans full-stack ETL development \nusing Python, Airflow, and Kafka; data modeling in Snowflake and Databricks; and deploying ML-driven \nscoring systems used in real-time workflows. \nKey Highlights: \n Reduced data delivery time by 50%+ through full-stack ETL pipelines built with Pyt",
    "n \nscoring systems used in real-time workflows. \nKey Highlights: \n Reduced data delivery time by 50%+ through full-stack ETL pipelines built with Python, SQL, Glue, \nand Kafka \n Enabled real-time credit scoring and risk forecasting by deploying ML models in production with \nSpark and SageMaker \n Built stakeholder-facing dashboards in Power BI and Tableau to support operational KPIs and \nstrategic visibility \n Automated compliance and reconciliation processes, cutting manual workload by 80% and \nimproving audit accuracy \n Implemented CI/CD pipelines using GitHub Actions and Terraform for secure, scalable, and testable \ndeployments \n Collaborated cross-functionally with product, finance, and compliance teams to ensure alignment \nwith business goals \nFocused on building data systems that are fast, explainable, and deeply embedded in how organizations \nmake decisions driving not just i",
    "siness goals \nFocused on building data systems that are fast, explainable, and deeply embedded in how organizations \nmake decisions driving not just insights, but outcomes. \nTECHNICAL SKILLS \nLanguages & Tools: Python (pandas, NumPy, Dash, scikit-learn), SQL, Java, R, PL/SQL, CSS, HTML, \nJavaScript \nData Engineering: ETL Pipelines (Python, SQL, SSIS), Snowflake, Kafka, Spark, Hive, YAML, Great \nExpectations \nAnalytics & Visualization: Power BI, Tableau, Excel, SSRS, Seaborn, Matplotlib \nCloud & Infra: AWS (EC2, Lambda, S3), Oracle Cloud, Databricks, GitHub, Grafana, Kibana \nModeling & ML: XGBoost, SHAP, Logistic Regression, Time Series (ARIMA, Prophet), Causal Inference \n(CausalForest) \nOther: REST APIs, Spring Boot, FastAPI, Feature Engineering, Data Vault, Agile, CI/CD Collaboration \n \nPROFESSIONAL EXPERIENCE \nCaliber Home Loans, Hyderabad May 2021 – Jul 2023 \nData Engineering \n Desig",
    "ering, Data Vault, Agile, CI/CD Collaboration \n \nPROFESSIONAL EXPERIENCE \nCaliber Home Loans, Hyderabad May 2021 – Jul 2023 \nData Engineering \n Designed and maintained Snowflake-based data pipelines aggregating mortgage data from origination, \nservicing, and underwriting systems, supporting downstream analytics and regulatory workflows. \n Developed Spring Boot microservices to handle real-time loan application processing, improving \nthroughput and reducing lag in the underwriting pipeline by 35%. \n Implemented pre-processing and validation layers in Python and SQL, reducing failed submissions due to \ndata quality issues by 18% month-over-month. \n Architected and deployed Snowflake Data Vault models to ensure traceability and compliance with \ninternal audit and financial regulations. \n Automated amortization and loan schedule generation using parameterized Python scripts, increasing",
    "with \ninternal audit and financial regulations. \n Automated amortization and loan schedule generation using parameterized Python scripts, increasing \nprocessing speed and reducing manual errors by over 70%. \n Developed Power BI dashboards for executives to monitor live KPIs such as delinquency rates, repayment \ntrends, and refinancing rates across portfolios. \n Integrated external credit scoring APIs into eligibility workflows, reducing risk misclassification rates and \nenhancing underwriting confidence. \n Built reconciliation systems comparing Snowflake data to monthly bank settlements, uncovering \nmismatches and reducing financial discrepancies by 95%. \n Collaborated with risk and finance teams to model prepayment risk, leveraging behavioral and \nmacroeconomic indicators to inform strategic lending decisions. \n Migrated legacy Excel/VBA-based reports to dynamic Python-based workf",
    "havioral and \nmacroeconomic indicators to inform strategic lending decisions. \n Migrated legacy Excel/VBA-based reports to dynamic Python-based workflows, cutting daily reporting time \nfrom hours to under 10 minutes. \n Led efforts to optimize Snowflake warehouse sizing and query efficiency, achieving an annual cost \nreduction exceeding ₹40 lakh ($50,000+). \n Automated hub-link-satellite schema generation using YAML-based configs, enabling rapid onboarding of \n20+ data sources within 2 months. \n Refactored batch ETL jobs into Kafka-driven microservices for scalable, event-based data ingestion, \ncutting end-to-end pipeline latency by over 50%. \n Implemented row- and column-level access policies in Snowflake, ensuring role-based data security and \naudit-compliant data views across departments. \n Deployed data quality checks using Great Expectations to enforce schema compliance and com",
    "rity and \naudit-compliant data views across departments. \n Deployed data quality checks using Great Expectations to enforce schema compliance and completeness \nthresholds, reducing downstream ETL failures. \n Trained and mentored junior data analysts on writing optimized SQL queries, building dashboards, and \nunderstanding mortgage lifecycle data. \n Partnered with IT, DevOps, and Compliance teams to redesign pipeline observability and incident response \nsystems using Power BI alerts and logs. \n Participated in quarterly roadmap sessions with data leaders and executive stakeholders, aligning \ninfrastructure improvements with strategic business goals. \n \nPROJECTS \nIntelligent Scheduling Risk System | Healthcare \nObjective: Reduce patient no-show losses across clinic network \n Engineered a no-show risk model using appointment, NOAA weather, ZIP-code census, and public transit \ndata \n B",
    "o-show losses across clinic network \n Engineered a no-show risk model using appointment, NOAA weather, ZIP-code census, and public transit \ndata \n Built features including forecasted weather, transit distance, and time-of-day impact \n Delivered XGBoost model with SHAP interpretability; embedded into overbooking simulation engine \n Quantified $1.2M in potential reclaimed revenue via optimized capacity planning \n Deployed risk dashboard in Power BI for COO and clinical operations team \n \nStrategic LTV Attribution & Segment Playbook | Fintech \nObjective: Predict customer lifetime value (LTV) and optimize growth levers \n Clustered customer behavior using KMeans and synthetic usage, support, and NPS data \n Forecasted LTV using survival regression and explained top drivers with SHAP \n Built per-segment growth playbooks (retention, upsell, expansion strategies) \n Simulated financial up",
    "gression and explained top drivers with SHAP \n Built per-segment growth playbooks (retention, upsell, expansion strategies) \n Simulated financial uplift; identified $2.7M ROI potential in Segment A via credit builder plan \n Delivered LTV heatmaps and executive-ready playbook slides for product/growth teams \n \nJourney Interruption Diagnostics | E-commerce \nObjective: Identify and fix UX drop-off points to recover revenue \n Parsed BigQuery logs from Google Analytics + CRM overlays to reconstruct user journeys \n Modeled abandonment causes across traffic sources, device types, and time-of-day \n Used anomaly detection and CausalForest modeling to estimate lift from UX fixes \n Identified Safari-specific modal issue costing $430K/month in lost conversions \n Delivered Tableau dashboard and CRO action plan to product leadership \n \nEDUCATION \nUniversity of Memphis, Memphis, TN \nMaster of S",
    "ost conversions \n Delivered Tableau dashboard and CRO action plan to product leadership \n \nEDUCATION \nUniversity of Memphis, Memphis, TN \nMaster of Science in Data Science, (Top 1% of class) \nRelevant Coursework: Advanced Database Management, Machine Learning, Deep Learning, MLOps, Cloud-\nBased AI Systems, Data Ethics and Governance, Research Methods and Analytics, Web and Social Media \nAnalytics, Predictive Modeling and Forecasting. \n \nCERTIFICATIONS \n Microsoft Certified: Power BI Data Analyst Associate \n Advanced Google Analytics \n Programming Essentials in Python - Cisco Networking Academy \n Mathematics for Machine Learning and Data Science - Coursera \n \nAWARDS & RECOGNITION \n Best AI Infrastructure Optimization Reduced model drift by 40 percent through AI observability \nimprovements \n Above & Beyond Award Delivered strategic projects on time with consistent quality and zero e",
    "t by 40 percent through AI observability \nimprovements \n Above & Beyond Award Delivered strategic projects on time with consistent quality and zero escalations"
  ]
}